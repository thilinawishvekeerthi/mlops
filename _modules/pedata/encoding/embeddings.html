<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
      <title>pedata.encoding.embeddings</title>
    
          <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../../../_static/theme.css " type="text/css" />
      
      <!-- sphinx script_files -->
        <script src="../../../_static/documentation_options.js?v=f9788867"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../../../_static/theme-vendors.js"></script> -->
      <script src="../../../_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="../../../genindex.html" />
  <link rel="search" title="Search" href="../../../search.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../../../index.html" class="home-link">
    
      <span class="site-name">pedata documentation</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../../../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../../../index.html#pedata-documentation">pedata documentation!</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 ">
            
              <a href="../../../pedata.html" class="reference internal ">pedata package</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../../../pedata.encoding.html" class="reference internal ">pedata.encoding package</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../../../pedata.config.html" class="reference internal ">pedata.config package</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../../../pedata.mutation.html" class="reference internal ">pedata.mutation package</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../../../modules.html" class="reference internal ">pedata</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../../../index.html">Docs</a> &raquo;</li>
    
      <li><a href="../../index.html">Module code</a> &raquo;</li>
    
    <li>pedata.encoding.embeddings</li>
  </ul>
  

  <ul class="page-nav">
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <h1>Source code for pedata.encoding.embeddings</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ankh</span>
<span class="kn">import</span> <span class="nn">esm</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Iterable</span>

<div class="viewcode-block" id="ESM">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.ESM">[docs]</a>
<span class="k">class</span> <span class="nc">ESM</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ESM (Evolutionary Scale Modeling) transformer for sequence transformation using a pre-trained model.</span>

<span class="sd">    This class provides a transformer interface to apply the ESM model for sequence transformation tasks.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; input_sequences = [&quot;ATGC&quot;, &quot;GCTA&quot;]</span>
<span class="sd">        &gt;&gt;&gt; esm = ESM()</span>
<span class="sd">        &gt;&gt;&gt; transformed_sequences = esm.transform(input_sequences)</span>
<span class="sd">        &gt;&gt;&gt; print(transformed_sequences[-1][-1][-1])</span>
<span class="sd">        -0.03196815401315689</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ESM.__init__">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.ESM.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the ESM transformer by loading the pre-trained model and its tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">alphabet</span> <span class="o">=</span> <span class="n">esm</span><span class="o">.</span><span class="n">pretrained</span><span class="o">.</span><span class="n">esm2_t6_8M_UR50D</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">alphabet</span><span class="o">.</span><span class="n">get_batch_converter</span><span class="p">()</span></div>


<div class="viewcode-block" id="ESM.fit">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.ESM.fit">[docs]</a>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the pre-trained model without modifications.</span>
<span class="sd">        It helps to adhere to the standard API of an SK learn transformer class</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input data, not used in this method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span></div>


<div class="viewcode-block" id="ESM.transform">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.ESM.transform">[docs]</a>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transforms the input sequences into their ESM representations.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input an iterable object of strings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Transformed representations of the input sequences as a nested list.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Check if X is not an iterable object of strings</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>  <span class="c1"># a string is iterable, so this is needed</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input X must be a non-empty list of strings.&quot;</span><span class="p">)</span>

        <span class="c1"># Convert the input sequences into a format compatible with the ESM model</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>

        <span class="c1"># Convert the sequence pairs into batch tokens using the tokenizer</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">batch_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Execute the pre-trained model with the batch tokens and retrieve representations from the 6th layer</span>
            <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
                <span class="n">batch_tokens</span><span class="p">,</span> <span class="n">repr_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">return_contacts</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

        <span class="c1"># Return the transformed representations</span>
        <span class="k">return</span> <span class="n">model_outputs</span><span class="p">[</span><span class="s2">&quot;representations&quot;</span><span class="p">][</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span></div>
</div>


<div class="viewcode-block" id="Ankh">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.Ankh">[docs]</a>
<span class="k">class</span> <span class="nc">Ankh</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ankh transformer for sequence transformation using a pre-trained model.</span>

<span class="sd">    The Ankh class provides a transformer interface to apply the Ankh model for sequence transformation tasks.</span>
<span class="sd">    It utilizes a pre-trained model and tokenizer to convert input sequences into their Ankh representations.</span>

<span class="sd">    Usage:</span>
<span class="sd">        ankh = Ankh()</span>
<span class="sd">        transformed_sequences = ankh.transform(input_sequences)</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; input_sequences = [&quot;MAPCT&quot;, &quot;KPGAT&quot;]</span>
<span class="sd">        &gt;&gt;&gt; ankh = Ankh()</span>
<span class="sd">        &gt;&gt;&gt; transformed_sequences = ankh.transform(input_sequences)</span>
<span class="sd">        &gt;&gt;&gt; print(transformed_sequences[-1][-1][-1])</span>
<span class="sd">        -0.010837498120963573</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Ankh.__init__">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.Ankh.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Ankh transformer by loading the pre-trained model and its tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">ankh</span><span class="o">.</span><span class="n">load_base_model</span><span class="p">()</span></div>


<div class="viewcode-block" id="Ankh.fit">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.Ankh.fit">[docs]</a>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the pre-trained model without modifications.</span>
<span class="sd">        This method is included to adhere to the standard API of an Scikit-learn transformer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input data, not used in this method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span></div>


<div class="viewcode-block" id="Ankh.transform">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.Ankh.transform">[docs]</a>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transforms the input sequences into their Ankh representations.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input an iterable object of strings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Transformed representations of the input sequences as a nested list.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Check if X is not an iterable object of strings</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>  <span class="c1"># a string is iterable, so this is needed</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input X must be a non-empty list of strings.&quot;</span><span class="p">)</span>

        <span class="c1"># Convert the input sequences into a format compatible with the Ankh model</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>

        <span class="c1"># Convert the sequence into batch tokens using the tokenizer</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">(</span>
            <span class="n">sequence</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Generate embeddings using the pre-trained model and input tokens</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># Return the transformed representations</span>
        <span class="k">return</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span></div>
</div>


<div class="viewcode-block" id="AnkhBatched">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.AnkhBatched">[docs]</a>
<span class="k">class</span> <span class="nc">AnkhBatched</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ankh batch transformer for sequence transformation using a pre-trained model.</span>

<span class="sd">    The Ankh class provides a transformer interface to apply the Ankh model for sequence transformation tasks.</span>
<span class="sd">    It utilizes a pre-trained model and tokenizer to convert input sequences into their Ankh representations.</span>

<span class="sd">    The difference between Ankh and AnkhBatched is that AnkhBatched sends a batch of sequences to Ankh model at once.</span>
<span class="sd">    It is faster than Ankh but requires more memory.</span>
<span class="sd">    Also the outputs are padded to the same length. This avoids the need for manually padding in the downstream tasks.</span>

<span class="sd">    Usage:</span>
<span class="sd">        ankh = Ankh()</span>
<span class="sd">        transformed_sequences = ankh.transform(input_sequences)</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; input_sequences = [&quot;MAPCT&quot;, &quot;KPGAT&quot;]</span>
<span class="sd">        &gt;&gt;&gt; ankh = Ankh()</span>
<span class="sd">        &gt;&gt;&gt; transformed_sequences = ankh.transform(input_sequences)</span>
<span class="sd">        &gt;&gt;&gt; print(transformed_sequences[-1][-1][-1])</span>
<span class="sd">        -0.010837498120963573</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="AnkhBatched.__init__">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.AnkhBatched.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Ankh transformer by loading the pre-trained model and its tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">ankh</span><span class="o">.</span><span class="n">load_base_model</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="AnkhBatched.fit">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.AnkhBatched.fit">[docs]</a>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the pre-trained model without modifications.</span>
<span class="sd">        This method is included to adhere to the standard API of an Scikit-learn transformer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input data, not used in this method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span></div>


<div class="viewcode-block" id="AnkhBatched.transform">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.AnkhBatched.transform">[docs]</a>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transforms the input sequences into their Ankh representations.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input an iterable object of strings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Transformed representations of the input sequences as a nested list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if X is not an iterable object of strings</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input X must be a non-empty list of strings.&quot;</span><span class="p">)</span>

        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">(</span>
            <span class="n">sequence</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="AnkhBatched.map_func">
<a class="viewcode-back" href="../../../pedata.encoding.html#pedata.encoding.embeddings.AnkhBatched.map_func">[docs]</a>
    <span class="k">def</span> <span class="nf">map_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This supports the batch encoding option of config.encoding_specs.add_encodings method.</span>
<span class="sd">        It is a wrapper for the transform method.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Input an iterable object of strings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Transformed representations of the input sequences as a nested list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&quot;aa_seq&quot;</span><span class="p">])</span></div>
</div>

</pre></div>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2023, Company.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 7.2.6 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.8.0.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>